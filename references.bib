%*******************************************************************
%   Visit below site for example on BibTex examples
%   https://verbosus.com/bibtex-style-examples.html
%   Most online databases have download-able (.bib) file 
%   Copy the content of the (.bib) and paste here
%   Use "\cite{Identifier}" to cite
%   @article{Tulabandhula, }, here "Tulabandhula" is the Identifier
%*******************************************************************
1_A Survey on RAG Meeting LLMs Towards Retrieval-Augmented Large Language Models

@misc{fan2024surveyragmeetingllms,
      title={A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models}, 
      author={Wenqi Fan and Yujuan Ding and Liangbo Ning and Shijie Wang and Hengyun Li and Dawei Yin and Tat-Seng Chua and Qing Li},
      year={2024},
      eprint={2405.06211},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.06211}, 
}


2_Advancing Legal Accessibility in Bangladesh through AI-Powered Assistance and Natural Language Interfaces



3_ASurvey on RAGMeetingLLMs Towards Retrieval-Augmented

@misc{fan2024surveyragmeetingllms,
      title={A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models}, 
      author={Wenqi Fan and Yujuan Ding and Liangbo Ning and Shijie Wang and Hengyun Li and Dawei Yin and Tat-Seng Chua and Qing Li},
      year={2024},
      eprint={2405.06211},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.06211}, 
}



4_Bridging Legal Knowledge and AI Retrieval Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization

@misc{barron2025bridginglegalknowledgeai,
      title={Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization}, 
      author={Ryan C. Barron and Maksim E. Eren and Olga M. Serafimova and Cynthia Matuszek and Boian S. Alexandrov},
      year={2025},
      eprint={2502.20364},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.20364}, 
}


5_Context Awareness Gate For Retrieval Augmented Generation

@misc{heydari2025contextawarenessgateretrieval,
      title={Context Awareness Gate For Retrieval Augmented Generation}, 
      author={Mohammad Hassan Heydari and Arshia Hemmat and Erfan Naman and Afsaneh Fatemi},
      year={2025},
      eprint={2411.16133},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.16133}, 
}



6_DistilBERT a distilled version of BERT smaller faster cheaper and lighter

@misc{sanh2020distilbertdistilledversionbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.01108}, 
}

7_Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization

@misc{barron2024domainspecificretrievalaugmentedgenerationusing,
      title={Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization}, 
      author={Ryan C. Barron and Ves Grantcharov and Selma Wanna and Maksim E. Eren and Manish Bhattarai and Nicholas Solovyev and George Tompkins and Charles Nicholas and Kim Ø. Rasmussen and Cynthia Matuszek and Boian S. Alexandrov},
      year={2024},
      eprint={2410.02721},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02721}, 
}


8_Enhanced Large Language Models-based Legal Query Responses through Retrieval Augmented Generation

@inproceedings{Li2024,
  title={Enhanced Large Language Models-based Legal Query Responses through Retrieval Augmented Generation},
  author={Ruiteng Li},
  year={2024},
  booktitle={Proceedings of the 2024 International Conference on Artificial Intelligence and Communication (ICAIC 2024)},
  pages={237-244},
  issn={1951-6851},
  isbn={978-94-6463-512-6},
  url={https://doi.org/10.2991/978-94-6463-512-6_27},
  doi={10.2991/978-94-6463-512-6_27},
  publisher={Atlantis Press}
}

9_Generative AI Systems in Legal Practice Offering Quality Legal Services while Upholding Legal Ethics

@article{terzidou2025generative,
  title={Generative AI Systems in Legal Practice Offering Quality Legal Services while Upholding Legal Ethics},
  author={Terzidou, Kalliopi},
  journal={International Journal of Law in Context},
  year={2025},
  pages={1--22},
  note={Available at SSRN: \url{https://ssrn.com/abstract=5197274}},
  doi={10.2139/ssrn.5197274}
}
10_LawPal  A Retrieval Augmented Generation Based System for Enhanced Legal Accessibility in India

@misc{panchal2025lawpalretrievalaugmented,
      title={LawPal : A Retrieval Augmented Generation Based System for Enhanced Legal Accessibility in India}, 
      author={Dnyanesh Panchal and Aaryan Gole and Vaibhav Narute and Raunak Joshi},
      year={2025},
      eprint={2502.16573},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.16573}, 
}


11_LexRAG Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal Consultation Conversation

@misc{li2025lexragbenchmarkingretrievalaugmentedgeneration,
      title={LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal Consultation Conversation}, 
      author={Haitao Li and Yifan Chen and Yiran Hu and Qingyao Ai and Junjie Chen and Xiaoyu Yang and Jianhui Yang and Yueyue Wu and Zeyang Liu and Yiqun Liu},
      year={2025},
      eprint={2502.20640},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.20640}, 
}


12_BengaliGPT An Instruction Following LLaMA Model for Bengali

@unknown{unknown,
author = {Singh, Guneet and Sen, Arghyadeep and Sekhar, Sambit and Sahoo, Shashikanta and Parida, Shantipriya and Dash, Satya and Bojar, Ondřej},
year = {2023},
month = {06},
pages = {},
title = {BengaliGPT: An Instruction Following LLaMA Model for Bengali},
doi = {10.13140/RG.2.2.16116.26247}
}

13_MobileBERT a Compact Task-Agnostic BERT for Resource-Limited Devices

@misc{sun2020mobilebertcompacttaskagnosticbert,
      title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices}, 
      author={Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},
      year={2020},
      eprint={2004.02984},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.02984}, 
}


14_RAAD LLM  Adaptive Anomaly Detection Using LLMs and RAG Integration

@misc{russellgilbert2025raadllmadaptiveanomalydetection,
      title={RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration}, 
      author={Alicia Russell-Gilbert and Sudip Mittal and Shahram Rahimi and Maria Seale and Joseph Jabour and Thomas Arnold and Joshua Church},
      year={2025},
      eprint={2503.02800},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.02800}, 
}


15_Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}



16_Sentence-BERT Sentence Embeddings using Siamese BERT-Networks

@misc{reimers2019sentencebertsentenceembeddingsusing,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.10084}, 
}


17_The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models

@inproceedings{10.1145/3644815.3644966,
author = {Yuan, Ye and Shi, Jiacheng and Zhang, Zongyao and Chen, Kaiwei and Zhang, Jingzhi and Stoico, Vincenzo and Malavolta, Ivano},
title = {The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644966},
doi = {10.1145/3644815.3644966},
abstract = {Context. While models like BERT and GPT are powerful, they require substantial resources. Knowledge distillation can be employed as a technique to enhance their efficiency. Yet, we lack a clear understanding on their performance and energy consumption. This uncertainty is a major concern, especially in practical applications, where these models could strain resources and limit accessibility for developers with limited means. Our drive also comes from the pressing need for environmentally-friendly and sustainable applications in light of growing environmental worries. To address this, it is crucial to accurately measure their energy consumption. Goal. This study aims to determine how Knowledge Distillation affects the energy consumption and performance of NLP models. Method. We benchmark BERT, Distilled-BERT, GPT-2, and Distilled-GPT-2 using three different tasks from 3 different categories selected from a third-party dataset. The energy consumption, CPU utilization, memory utilization, and inference time of the considered NLP models are measured and statistically analyzed.Results. We observed notable differences between the original and the distilled version of the measured NLP models. Distilled versions tend to consume less energy, while distilled GPT-2 uses less CPU. Conclusion. The results of this study highlight the critical impact of model choice on performance and energy consumption metrics. Future research should consider a wider range of distilled models, diverse benchmarks, and deployment environments, as well as explore the ecological footprint of these models, particularly in the context of environmental sustainability.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {129–133},
numpages = {5},
location = {Lisbon, Portugal},
series = {CAIN '24}
}


18_TinyBERT  Distilling BERT for Natural Language Understanding

@misc{jiao2020tinybertdistillingbertnatural,
      title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
      author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
      year={2020},
      eprint={1909.10351},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.10351}, 
}


19_Too Late to Train Too Early To Use A Study on Necessity and Viability of Low-Resource Bengali LLMs

@misc{mahfuz2024latetrainearlyuse,
      title={Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs}, 
      author={Tamzeed Mahfuz and Satak Kumar Dey and Ruwad Naswan and Hasnaen Adil and Khondker Salman Sayeed and Haz Sameen Shahgir},
      year={2024},
      eprint={2407.00416},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.00416}, 
}


20_Towards Efficient Educational Chatbots Benchmarking RAG Frameworks

@misc{khan2025efficienteducationalchatbotsbenchmarking,
      title={Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks}, 
      author={Umar Ali Khan and Ekram Khan and Fiza Khan and Athar Ali Moinuddin},
      year={2025},
      eprint={2503.00781},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2503.00781}, 
}
