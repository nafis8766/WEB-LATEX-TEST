\section{Preliminaries}

This section will give the important background information related to the application of AI to the field of law and especially in Bangladesh in this research. It suggests Large Language Models (LLMs) and the issue of hallucinations. Retrieval-Augmented Generation (RAG) architecture is described, which works in indexing, retrieval and generation steps to enhance factual accuracy. These are the preparation of legal information, its vectorization representations, search of the relevant data based on the user query, and finally, generation of valid answers using the retrieved context. The peculiarities of the situation in Bangladesh the low level of digitalization, unreliable internet connection, and Bangla/English hybrid legal terminology require a tailored, offline-first AI solution. The keys to filtering the information are Context-Aware Gate (CAG) and Natural Language Processing (NLP) methods, and models such as DPR and SBERT need to be fine-tuned on the Bangla legal texts to make them relevant and avoid hallucinations. It is aimed at legal aid using simple mobile phones without continuous internet connection. The two methods are essential in the creation of small, energy-conscious, and usable legal AI applications that will suit local users.

\section{Review of Existing Research}
A growing interest in Artificial Intelligence (AI) has led to great changes in multiple professional areas, and the legal sector has been affected more than most. AI's capabilities are increasingly harnessed to augment legal research, automate the generation of intricate legal documents, and enhance the nuanced comprehension of complex legislative frameworks and policy directives. Through AI, it takes less time for researchers to find thorough information among abundant court records, journal articles, and laws. Apart from gathering information, AI systems are of value in policy analysis by assisting in catching missed opinions and clarifying parts of regulatory documents using complex natural language methods. Even with all these impressive capabilities, there are many difficulties and limits to which AI is being applied in the legal sector \cite{terzidou2025generative}.

At present, AI models (especially LLMs) are not properly equipped to understand the main ideas and ethical aspects of legal rules. Unfortunately, this drawback could lead to wrong conclusions or details that are not tolerated in this field \cite{terzidou2025generative}. LLMs are notably susceptible to ``hallucination'' a phenomenon where they confidently produce factually incorrect or entirely fabricated information that, deceptively, appears plausible \cite{terzidou2025generative}. Because of its tendency to make unreliable decisions, AI is not trusted and useful in areas that need high accuracy and responsibility. As a result, research and development should be carried out on a long term basis to strengthen, trust, and ethically manage AI applications used in the legal sector. So far, local use and adaptation of advanced legal AI technologies are not widely available in Bangladesh, which underlines the need for their focused development to fit the country’s special needs .

A particular Natural Language Processing (NLP)  architecture called Retrieval Augmented Generation (RAG) was invented to improve the factual accuracy of LLM results \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}. With this process, LLMs mix text generation and the retrieval of documents in a new way. Unlike depending just on rigid knowledge from their training stage, RAG keeps looking for new information from an open database that is updated regularly \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}. This retrieved data is then used to ground the LLM’s responses, a critical mechanism that significantly mitigates the risk of hallucinations, a pervasive challenge with large language models \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}\cite{fan2024surveyragmeetingllms}.

The RAG model usually works in three well planned steps: indexing, retrieval, and generation \cite{fan2024surveyragmeetingllms}. In the indexing phase, large amounts of raw legal information, including statutes, decisions from courts, and legal research papers, are prepared first by cleaning them and breaking them into suitable sections, and then using powerful models such as BGE or GTE to create vector representations of each piece of data \cite{li2025lexragbenchmarkingretrievalaugmentedgeneration}\cite{khan2025efficienteducationalchatbotsbenchmarking}. After that, these numbers are efficiently placed in databases such as FAISS or Weaviate, making it easy to perform fast similarity searches \cite{panchal2025lawpalretrievalaugmented}\cite{khan2025efficienteducationalchatbotsbenchmarking}. For the next step, a user’s words are again processed, and a retriever component (such as BM25 if it is sparse or DPR or SBERT if it is dense) looks for the most meaningful information in the vector database. Many times, this process includes expanding or rewording the user’s search query before retrieving context to help increase its relevance and quality \cite{fan2024surveyragmeetingllms}\cite{heydari2025contextawarenessgateretrieval}. Finally, in the generation stage, these precisely retrieved documents are concatenated with the original user query to form an ``augmented prompt'', which is then fed into a generative LLM. Because of its training on different contexts, the LLM can give a relevant and accurate answer to each question it receives \cite{fan2024surveyragmeetingllms}\cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}. For example, RAG-Sequence and RAG-Token are ways of forming RAG models; the first uses the same document to predict all the output, but the latter enables the use of several documents, each for a specific output token, providing a better output and finer accuracy \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}.

RAG successfully maximizes the quality of answers when precise and suitable information is crucial in the field of law. Even so, common RAG systems tend to miss the mark, as they do not deeply grasp the topics in highly technical fields like law, since its terms are frequently very complex and come from various languages \cite{barron2025bridginglegalknowledgeai}\cite{,barron2024domainspecificretrievalaugmentedgenerationusing}\cite{panchal2025lawpalretrievalaugmented}. To overcome this problem, people have fashioned domain-specific RAG frameworks. By way of example, LexRAG points out that the use of customized RAG systems is vital for the legal domain of multi-turn consultation conversations. This system tries to retrieve the most suitable legal articles and relies on citation information to make sure its responses are also verifiable \cite{li2025lexragbenchmarkingretrievalaugmentedgeneration}. It also evaluates the system's ability to handle complex multi-turn dialogues, which involve progressively unfolding issues, pronoun resolution, and abrupt topic shifts all common in real-world legal consultations \cite{li2025lexragbenchmarkingretrievalaugmentedgeneration}.

Given the special features of the Bangladeshi legal field, a system that is very specific to that environment is essential. For this reason, the pipeline’s retriever and generator need extra attention to improve functionality. Arranging for important legal resources in both Bangla and English supports the system in interpreting and delivering the right information for the community. Embedding models such as SBERT or MiniLM are helpful to accurately spot similar words and they can be explicitly trained with data that matters for the task. After that, such tools can be matched with a CAG module to achieve even better performance. With the help of the CAG, the RAG system can evaluate which parts of the retrieved information are useful and which are not, and removes extra or mistaken data to provide more accurate results \cite{sanh2020distilbertdistilledversionbert}. It is particularly important to use this three-step strategy in Bangladesh to make sure the AI assistant is reliable, practical, and effective for law practice.

Bangladesh deals with special challenges linked to society, technology, language, and facilities, so the need is for a customized AI system to address these issues. These challenges include, but are not limited to, the limited accessibility of modern digital tools and resources, the inconsistent internet connectivity prevalent across many rural and even urban areas, and the widespread use of legal documents that often comprise a complex mix of Bangla and English terminologies, reflecting the country's unique linguistic and historical heritage \cite{mahfuz2024latetrainearlyuse}. It is absolutely necessary to make a legal AI tool that works well offline and is very intelligent, in order to manage these complicated issues. CAG (Context-Aware Gate) and NLP (Natural Language Processing) techniques are included in this tool to thoroughly filter both unnecessary and factually wrong information, which is essential for legal cases’ validity.

Due to the complicated nature of Bangla legal materials and the lack of comprehensive and fine-quality data, most LLMs and traditional systems often fail to do well. Therefore, models named Dense Passage Retriever (DPR) and Sentence-BERT (SBERT) are to be retrained or fine tuned carefully on Bangla law texts to help them understand and search within that area at a better level \cite{reimers2019sentencebertsentenceembeddingsusing}. These models produce results, and these are carefully checked by the Context-Aware Gate (CAG) module. Part of the CAG’s job is to ensure the information collected is still relevant and legal, so only suitable and correct data is used. It is necessary for this care in sorting through queries to prevent hallucinations and make sure the answers are accurate. The system’s main objective is to deliver legal help with a call or message through a basic mobile phone and do it without requiring constant internet access. This ``offline-first'' approach is designed to democratize access to vital legal assistance, making it readily available to a broader population in Bangladesh, particularly those in underserved areas where internet access is intermittent or non-existent.

Because resource-constrained devices are often used locally, using KD and Quantization techniques with sophisticated ML models is highly necessary. To deal with the tough challenges brought by large language models, these engineering techniques are vital.

Knowledge Distillation (KD) is a potent model compression technique where the accumulated knowledge from a large, high-performing, but computationally intensive ``teacher'' model is systematically transferred to a smaller, more efficient ``student'' model. The main aim of KD is to let the student model match the teacher in performance, while it uses much less computing power, needs less memory, and is faster to apply. This knowledge transfer can manifest in several ways: the student might learn to mimic the teacher's probability distributions over outputs (soft targets), replicate its internal activations or feature maps (intermediate representations), or even imitate its attention patterns within transformer layers. While this process can result in a marginal, often negligible, compromise in the student's overall performance, the gains in efficiency across various metrics including reduced storage requirements, accelerated processing, and lower energy consumption are substantial, rendering the student model far more practical for real world deployment. Among successful cases of this technique, we have DistilBERT \cite{sanh2020distilbertdistilledversionbert}, TinyBERT \cite{jiao2020tinybertdistillingbertnatural}, and MobileBERT \cite{sun2020mobilebertcompacttaskagnosticbert}. It is commonly known that DistilBERT has a better energy consumption rate than BERT, so using it saves resources in applications that have limited resources \cite{10.1145/3644815.3644966}. To overcome the issues of data lack and limited computing power in legal AI systems, KD is very important for compact and efficient Bangla LLMs.

The quantization model compression method aims to cut down the precision in the model’s parameters  activations. In order to reduce size, a model’s values can sometimes be represented in fewer bits, for example, choosing INT4 over FP32 . Besides, it can remarkably improve inference performance, most notably in cases where low-precision arithmetic is possible on the hardware. Highly compressing models, achieving quick processing, saving power, and deploying LLMs on basic mobile devices, edge servers, and everyday computers are the leading goals of quantization. Quantization takes many forms, and Post-Training Quantization (PTQ) is one of them. It is applied after training, while Quantization-Aware Training (QAT) involves simulating quantization as part of the training procedure. Just like QAT, the QLORA approach using PEFT emphasizes how little memory can be needed for training, while maintaining performance, because it fine tunes 4-bit quantized base models. Quantization does not provide the same level of accuracy for every model or task; kindest always leads to a bigger decrease in the accuracy of models that need complex reasoning or work with several languages. Yet, with regard to Bangla NLP, effective use of quantization together with KD and pruning has achieved good results in emotion classification and greatly helps in obtaining efficient and lightweight legal AI solutions for local users. The use of KD together with quantization results in highly practical models that are good at saving energy.

Though multilingual LLMs advance rapidly, it is still very important to create tailored models for the Bengali language \cite{mahfuz2024latetrainearlyuse}. While general-purpose LLMs can work in many areas, they usually do not have enough knowledge of Bengali and its cultural details. Studies have carefully pointed out various significant issues that come with building instruction-following Bengali LLMs. Most of these models find it difficult to process general matters and remember things, mainly because of the scarcity of good domain-specific data in Bengali \cite{mahfuz2024latetrainearlyuse}\cite{khan2025efficienteducationalchatbotsbenchmarking}.

An important difficulty is that existing LLMs trained for English are not very efficient at handling Bengali text. Most LLMs utilize Byte-Pair Encoding (BPE) or similar subword tokenization algorithms, which, when applied to morphologically rich languages like Bengali, tend to ``over-tokenize'' As a result, Bengali usually needs more small constituents (subword tokens) than English, for example, it has around 0.85 characters per token while English only has 4.5. By over-tokenizing, the length of sequences increases a lot, which results in more computation being needed and also leads to lower accuracy and efficiency \cite{mahfuz2024latetrainearlyuse}. Special aspects of Bengali writing, such as clusters of consonants and odd distribution of empty space, add more difficulties during the process of tokenization . That is why, for Bengali NLG, popular LLMs GPT-4 and LLaMA-3 tend to fall short, proving that creating specialized Bengali LLMs is a must because of their precise tokenizers and approaches \cite{mahfuz2024latetrainearlyuse}\cite{unknown}.

It is evident from additional sources that the process of creating strong Bengali LLMs is very costly and challenging. The main reasons behind this challenge are a lack of high-quality data for Bengali, traditional evaluation methods that miss its special characteristics, and not having much fine-tuning information to help study \cite{mahfuz2024latetrainearlyuse}. All these results show that there is a strong need for lightweight, purposeful, and mindful legal AI systems that are specially designed for places like Bangladesh. BengaliGPT \cite{unknown}, TituLLMs, and TigerLLM are some of the actively working initiatives in Bangladesh that help develop Bangla LLMs. In these projects, the teams use carefully chosen, culturally appropriate datasets and come up with tokenizer adjustments for each language to improve the models’ results, regardless of their size. Such dedication is meant to address two major issues: the lack of quality Bangla digital text corpus and the over-tokenizing effect found in tokenizers designed mainly for English. Moving forward, Bangla LLMs will concentrate on increasing their data, inventing better and culturally welcoming tokenization ways, strengthening the models, and compressing them for easier rollouts. Adopting this method ensures that LLMs play a positive role and are accepted, mainly in the field of legal AI in Bangladesh.

\section{Summary of Key Findings}

The current literature review demonstrates the transformative power of AI in the legal field, especially regarding research and document creation, though it also serious drawback of LLM hallucinations. One of the essential mitigation strategies is considered Retrieval-Augmented Generation (RAG), which enhances accuracy at the stages of indexing, retrieval, and generation. Its specificities, such as poor digital access and unreliable internet connection, as well as the existence of mixed-language legal documents, highlight the necessity of specialized, offline-first AI tools that would utilize Context-Aware Gate (CAG) and NLP approaches and Bangla legal text fine-tuning models. Knowledge Distillation (KD) and Quantization are types of model compression methods that are essential to making such complex LLMs useful on resource-limited devices, making them efficient and practical. In the end, the review highlights the high necessity of domain-specific, light, and context-aware Bengali LLMs to surpass language peculiarities and data-sparseness and introduce a successful AI usage in the Bangladeshi legal sector.



